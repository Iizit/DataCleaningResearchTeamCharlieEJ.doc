{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNl+FnQlLMvkcxD9QHi3qMQ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Iizit/DataCleaningResearchTeamCharlieEJ.doc/blob/main/restart_of_lifebrear.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Wiqhtn9GuMgX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7d7c3e08-6dd0-4230-b470-8604aadca9c5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   id    login_id              mail_address                          password  \\\n",
            "0   1    sugimoto   sugimoto@lifebear.co.jp  f0bac04aa1b45cf443d722d6f71c0250   \n",
            "1   2         kou  nakanishi@lifebear.co.jp  48207c322ee5bb156ffec9f08c960aaa   \n",
            "2   3      yusuke     yuozawa1208@gmail.com  048261a8024ce51d379eb53cc51aaf33   \n",
            "3   4  entyan1106        endo1106@gmail.com  cd77a9dac26260a104facda5665eb3ab   \n",
            "4   5      kuriki          kuriki@wavy4.com  a026597c294cc48cd20ae361f10cbab1   \n",
            "\n",
            "            created_at          salt birthday_on  gender  \n",
            "0  2012-01-13 22:54:05  yGwBKynnsctI  1984-11-09     0.0  \n",
            "1  2012-01-14 12:48:31  aha6EuRYCDvU  1986-11-13     0.0  \n",
            "2  2012-01-17 15:33:22  PVS59dPWk9BH  1984-12-08     0.0  \n",
            "3  2012-01-17 15:37:02  vLZI6TVCJowN  1987-11-06     0.0  \n",
            "4  2012-01-17 18:52:32  swFznWWk79fg  1986-10-21     0.0  \n"
          ]
        }
      ],
      "source": [
        "# prompt: read csv \"/content/3.6M-Japan-lifebear.com-Largest-Notebook-App-UsersDB-csv-2019.csv\" print head (first 5) and change the separator to a ';', set to low memory = 'True'. can you help me refine this prompt so that the ai separates the information in the correct columns?\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "# Read the CSV file with specified separator and low_memory option\n",
        "df = pd.read_csv('/content/3.6M-Japan-lifebear.com-Largest-Notebook-App-UsersDB-csv-2019.csv', sep=';', low_memory=True)\n",
        "\n",
        "# Print the first 5 rows of the DataFrame\n",
        "print(df.head())\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt:  Create a function to split /content/JapanLifeBar.csv into 4 chunks with parameters for the file_path, chunk_size and output_directory. Include error checking. chunk_size should be by rows.  and export the chunks to a csv\n",
        "\n",
        "import pandas as pd\n",
        "import os\n",
        "\n",
        "\n",
        "def split_csv(file_path, chunk_size, output_directory):\n",
        "    \"\"\"\n",
        "    Splits a CSV file into multiple chunks based on the specified chunk size.\n",
        "\n",
        "    Args:\n",
        "        file_path (str): Path to the input CSV file.\n",
        "        chunk_size (int): Number of rows per chunk.\n",
        "        output_directory (str): Directory to save the output chunks.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        if not os.path.exists(output_directory):\n",
        "            os.makedirs(output_directory)\n",
        "\n",
        "        df = pd.read_csv(file_path, sep=';', low_memory=True, encoding='latin-1')  # Specify encoding if needed\n",
        "\n",
        "        num_chunks = (len(df) // chunk_size) + 1\n",
        "\n",
        "        for i in range(num_chunks):\n",
        "            start_row = i * chunk_size\n",
        "            end_row = min((i + 1) * chunk_size, len(df))\n",
        "            chunk = df.iloc[start_row:end_row]\n",
        "\n",
        "            output_file = os.path.join(output_directory, f\"chunk_{i+1}.csv\")\n",
        "            chunk.to_csv(output_file, index=False)\n",
        "\n",
        "            print(f\"Chunk {i+1} saved to: {output_file}\")\n",
        "\n",
        "    except FileNotFoundError:\n",
        "        print(f\"Error: File not found at {file_path}\")\n",
        "    except Exception as e:\n",
        "        print(f\"An error occurred: {e}\")\n",
        "\n",
        "\n",
        "# Example usage:\n",
        "file_path = \"/content/3.6M-Japan-lifebear.com-Largest-Notebook-App-UsersDB-csv-2019.csv\"\n",
        "chunk_size = 500000  # 1 million rows per chunk\n",
        "output_directory = \"/content/chunks\"\n",
        "\n",
        "split_csv(file_path, chunk_size, output_directory)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "COVAkFgUvsFs",
        "outputId": "a7d782e0-9bc0-48ed-8da5-c0b9e0944fc7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-3-409140bef977>:20: DtypeWarning: Columns (6) have mixed types. Specify dtype option on import or set low_memory=False.\n",
            "  df = pd.read_csv(file_path, sep=';', low_memory=True, encoding='latin-1')  # Specify encoding if needed\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Chunk 1 saved to: /content/chunks/chunk_1.csv\n",
            "Chunk 2 saved to: /content/chunks/chunk_2.csv\n",
            "Chunk 3 saved to: /content/chunks/chunk_3.csv\n",
            "Chunk 4 saved to: /content/chunks/chunk_4.csv\n",
            "Chunk 5 saved to: /content/chunks/chunk_5.csv\n",
            "Chunk 6 saved to: /content/chunks/chunk_6.csv\n",
            "Chunk 7 saved to: /content/chunks/chunk_7.csv\n",
            "Chunk 8 saved to: /content/chunks/chunk_8.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: update this code to check for missing values\n",
        "\n",
        "import pandas as pd\n",
        "import os\n",
        "\n",
        "\n",
        "# Read the CSV file with specified separator and low_memory option\n",
        "df = pd.read_csv('/content/3.6M-Japan-lifebear.com-Largest-Notebook-App-UsersDB-csv-2019.csv', sep=';', low_memory=True)\n",
        "\n",
        "# Print the first 5 rows of the DataFrame\n",
        "print(df.head())\n",
        "\n",
        "\n",
        "# Check for missing values in the DataFrame\n",
        "missing_values = df.isnull().sum()\n",
        "print(\"\\nMissing values per column:\\n\", missing_values)\n",
        "\n",
        "\n",
        "def split_csv(file_path, chunk_size, output_directory):\n",
        "    \"\"\"\n",
        "    Splits a CSV file into multiple chunks based on the specified chunk size.\n",
        "\n",
        "    Args:\n",
        "        file_path (str): Path to the input CSV file.\n",
        "        chunk_size (int): Number of rows per chunk.\n",
        "        output_directory (str): Directory to save the output chunks.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        if not os.path.exists(output_directory):\n",
        "            os.makedirs(output_directory)\n",
        "\n",
        "        df = pd.read_csv(file_path, sep=';', low_memory=True, encoding='latin-1')  # Specify encoding if needed\n",
        "\n",
        "        # Check for missing values in the DataFrame before splitting\n",
        "        missing_values = df.isnull().sum()\n",
        "        print(\"\\nMissing values per column:\\n\", missing_values)\n",
        "\n",
        "        num_chunks = (len(df) // chunk_size) + 1\n",
        "\n",
        "        for i in range(num_chunks):\n",
        "            start_row = i * chunk_size\n",
        "            end_row = min((i + 1) * chunk_size, len(df))\n",
        "            chunk = df.iloc[start_row:end_row]\n",
        "\n",
        "            output_file = os.path.join(output_directory, f\"chunk_{i+1}.csv\")\n",
        "            chunk.to_csv(output_file, index=False)\n",
        "\n",
        "            print(f\"Chunk {i+1} saved to: {output_file}\")\n",
        "\n",
        "    except FileNotFoundError:\n",
        "        print(f\"Error: File not found at {file_path}\")\n",
        "    except Exception as e:\n",
        "        print(f\"An error occurred: {e}\")\n",
        "\n",
        "\n",
        "# Example usage:\n",
        "file_path = \"/content/3.6M-Japan-lifebear.com-Largest-Notebook-App-UsersDB-csv-2019.csv\"\n",
        "chunk_size = 500000  # 1 million rows per chunk\n",
        "output_directory = \"/content/chunks\"\n",
        "\n",
        "split_csv(file_path, chunk_size, output_directory)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j0XlneUEZ2-X",
        "outputId": "81082089-a7e9-459c-9f2e-b8372e9c51e0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-4-7042ad5de939>:8: DtypeWarning: Columns (6) have mixed types. Specify dtype option on import or set low_memory=False.\n",
            "  df = pd.read_csv('/content/3.6M-Japan-lifebear.com-Largest-Notebook-App-UsersDB-csv-2019.csv', sep=';', low_memory=True)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   id    login_id              mail_address                          password  \\\n",
            "0   1    sugimoto   sugimoto@lifebear.co.jp  f0bac04aa1b45cf443d722d6f71c0250   \n",
            "1   2         kou  nakanishi@lifebear.co.jp  48207c322ee5bb156ffec9f08c960aaa   \n",
            "2   3      yusuke     yuozawa1208@gmail.com  048261a8024ce51d379eb53cc51aaf33   \n",
            "3   4  entyan1106        endo1106@gmail.com  cd77a9dac26260a104facda5665eb3ab   \n",
            "4   5      kuriki          kuriki@wavy4.com  a026597c294cc48cd20ae361f10cbab1   \n",
            "\n",
            "            created_at          salt birthday_on  gender  \n",
            "0  2012-01-13 22:54:05  yGwBKynnsctI  1984-11-09     0.0  \n",
            "1  2012-01-14 12:48:31  aha6EuRYCDvU  1986-11-13     0.0  \n",
            "2  2012-01-17 15:33:22  PVS59dPWk9BH  1984-12-08     0.0  \n",
            "3  2012-01-17 15:37:02  vLZI6TVCJowN  1987-11-06     0.0  \n",
            "4  2012-01-17 18:52:32  swFznWWk79fg  1986-10-21     0.0  \n",
            "\n",
            "Missing values per column:\n",
            " id                    0\n",
            "login_id              2\n",
            "mail_address          0\n",
            "password              0\n",
            "created_at            0\n",
            "salt                  0\n",
            "birthday_on     3294901\n",
            "gender          1317523\n",
            "dtype: int64\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-4-7042ad5de939>:32: DtypeWarning: Columns (6) have mixed types. Specify dtype option on import or set low_memory=False.\n",
            "  df = pd.read_csv(file_path, sep=';', low_memory=True, encoding='latin-1')  # Specify encoding if needed\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Missing values per column:\n",
            " id                    0\n",
            "login_id              2\n",
            "mail_address          0\n",
            "password              0\n",
            "created_at            0\n",
            "salt                  0\n",
            "birthday_on     3294901\n",
            "gender          1317523\n",
            "dtype: int64\n",
            "Chunk 1 saved to: /content/chunks/chunk_1.csv\n",
            "Chunk 2 saved to: /content/chunks/chunk_2.csv\n",
            "Chunk 3 saved to: /content/chunks/chunk_3.csv\n",
            "Chunk 4 saved to: /content/chunks/chunk_4.csv\n",
            "Chunk 5 saved to: /content/chunks/chunk_5.csv\n",
            "Chunk 6 saved to: /content/chunks/chunk_6.csv\n",
            "Chunk 7 saved to: /content/chunks/chunk_7.csv\n",
            "Chunk 8 saved to: /content/chunks/chunk_8.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: update this code to find duplicates and romove them\n",
        "\n",
        "import pandas as pd\n",
        "import os\n",
        "\n",
        "\n",
        "# Read the CSV file with specified separator and low_memory option\n",
        "df = pd.read_csv('/content/3.6M-Japan-lifebear.com-Largest-Notebook-App-UsersDB-csv-2019.csv', sep=';', low_memory=True)\n",
        "\n",
        "# Print the first 5 rows of the DataFrame\n",
        "print(df.head())\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def split_csv(file_path, chunk_size, output_directory):\n",
        "    \"\"\"\n",
        "    Splits a CSV file into multiple chunks based on the specified chunk size.\n",
        "\n",
        "    Args:\n",
        "        file_path (str): Path to the input CSV file.\n",
        "        chunk_size (int): Number of rows per chunk.\n",
        "        output_directory (str): Directory to save the output chunks.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        if not os.path.exists(output_directory):\n",
        "            os.makedirs(output_directory)\n",
        "\n",
        "        df = pd.read_csv(file_path, sep=';', low_memory=True, encoding='latin-1')  # Specify encoding if needed\n",
        "\n",
        "        # Remove duplicate rows based on all columns\n",
        "        df = df.drop_duplicates()\n",
        "\n",
        "        num_chunks = (len(df) // chunk_size) + 1\n",
        "\n",
        "        for i in range(num_chunks):\n",
        "            start_row = i * chunk_size\n",
        "            end_row = min((i + 1) * chunk_size, len(df))\n",
        "            chunk = df.iloc[start_row:end_row]\n",
        "\n",
        "            output_file = os.path.join(output_directory, f\"chunk_{i+1}.csv\")\n",
        "            chunk.to_csv(output_file, index=False)\n",
        "\n",
        "            print(f\"Chunk {i+1} saved to: {output_file}\")\n",
        "\n",
        "    except FileNotFoundError:\n",
        "        print(f\"Error: File not found at {file_path}\")\n",
        "    except Exception as e:\n",
        "        print(f\"An error occurred: {e}\")\n",
        "\n",
        "\n",
        "# Example usage:\n",
        "file_path = \"/content/3.6M-Japan-lifebear.com-Largest-Notebook-App-UsersDB-csv-2019.csv\"\n",
        "chunk_size = 500000  # 1 million rows per chunk\n",
        "output_directory = \"/content/chunks\"\n",
        "\n",
        "split_csv(file_path, chunk_size, output_directory)\n",
        "\n",
        "\n",
        "\n",
        "# Read the CSV file with specified separator and low_memory option\n",
        "df = pd.read_csv('/content/3.6M-Japan-lifebear.com-Largest-Notebook-App-UsersDB-csv-2019.csv', sep=';', low_memory=True)\n",
        "\n",
        "# Print the first 5 rows of the DataFrame\n",
        "print(df.head())\n",
        "\n",
        "\n",
        "# Check for missing values in the DataFrame\n",
        "missing_values = df.isnull().sum()\n",
        "print(\"\\nMissing values per column:\\n\", missing_values)\n",
        "\n",
        "\n",
        "def split_csv(file_path, chunk_size, output_directory):\n",
        "    \"\"\"\n",
        "    Splits a CSV file into multiple chunks based on the specified chunk size.\n",
        "\n",
        "    Args:\n",
        "        file_path (str): Path to the input CSV file.\n",
        "        chunk_size (int): Number of rows per chunk.\n",
        "        output_directory (str): Directory to save the output chunks.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        if not os.path.exists(output_directory):\n",
        "            os.makedirs(output_directory)\n",
        "\n",
        "        df = pd.read_csv(file_path, sep=';', low_memory=True, encoding='latin-1')  # Specify encoding if needed\n",
        "\n",
        "        # Remove duplicate rows based on all columns\n",
        "        df = df.drop_duplicates()\n",
        "\n",
        "        # Check for missing values in the DataFrame before splitting\n",
        "        missing_values = df.isnull().sum()\n",
        "        print(\"\\nMissing values per column:\\n\", missing_values)\n",
        "\n",
        "        num_chunks = (len(df) // chunk_size) + 1\n",
        "\n",
        "        for i in range(num_chunks):\n",
        "            start_row = i * chunk_size\n",
        "            end_row = min((i + 1) * chunk_size, len(df))\n",
        "            chunk = df.iloc[start_row:end_row]\n",
        "\n",
        "            output_file = os.path.join(output_directory, f\"chunk_{i+1}.csv\")\n",
        "            chunk.to_csv(output_file, index=False)\n",
        "\n",
        "            print(f\"Chunk {i+1} saved to: {output_file}\")\n",
        "\n",
        "    except FileNotFoundError:\n",
        "        print(f\"Error: File not found at {file_path}\")\n",
        "    except Exception as e:\n",
        "        print(f\"An error occurred: {e}\")\n",
        "\n",
        "\n",
        "# Example usage:\n",
        "file_path = \"/content/3.6M-Japan-lifebear.com-Largest-Notebook-App-UsersDB-csv-2019.csv\"\n",
        "chunk_size = 500000  # 1 million rows per chunk\n",
        "output_directory = \"/content/chunks\"\n",
        "\n",
        "split_csv(file_path, chunk_size, output_directory)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hENuNdUjaPnr",
        "outputId": "1c7da318-6af1-4d98-8067-af2dc6518e48"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-5-868cbddd9b93>:8: DtypeWarning: Columns (6) have mixed types. Specify dtype option on import or set low_memory=False.\n",
            "  df = pd.read_csv('/content/3.6M-Japan-lifebear.com-Largest-Notebook-App-UsersDB-csv-2019.csv', sep=';', low_memory=True)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   id    login_id              mail_address                          password  \\\n",
            "0   1    sugimoto   sugimoto@lifebear.co.jp  f0bac04aa1b45cf443d722d6f71c0250   \n",
            "1   2         kou  nakanishi@lifebear.co.jp  48207c322ee5bb156ffec9f08c960aaa   \n",
            "2   3      yusuke     yuozawa1208@gmail.com  048261a8024ce51d379eb53cc51aaf33   \n",
            "3   4  entyan1106        endo1106@gmail.com  cd77a9dac26260a104facda5665eb3ab   \n",
            "4   5      kuriki          kuriki@wavy4.com  a026597c294cc48cd20ae361f10cbab1   \n",
            "\n",
            "            created_at          salt birthday_on  gender  \n",
            "0  2012-01-13 22:54:05  yGwBKynnsctI  1984-11-09     0.0  \n",
            "1  2012-01-14 12:48:31  aha6EuRYCDvU  1986-11-13     0.0  \n",
            "2  2012-01-17 15:33:22  PVS59dPWk9BH  1984-12-08     0.0  \n",
            "3  2012-01-17 15:37:02  vLZI6TVCJowN  1987-11-06     0.0  \n",
            "4  2012-01-17 18:52:32  swFznWWk79fg  1986-10-21     0.0  \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-5-868cbddd9b93>:29: DtypeWarning: Columns (6) have mixed types. Specify dtype option on import or set low_memory=False.\n",
            "  df = pd.read_csv(file_path, sep=';', low_memory=True, encoding='latin-1')  # Specify encoding if needed\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Chunk 1 saved to: /content/chunks/chunk_1.csv\n",
            "Chunk 2 saved to: /content/chunks/chunk_2.csv\n",
            "Chunk 3 saved to: /content/chunks/chunk_3.csv\n",
            "Chunk 4 saved to: /content/chunks/chunk_4.csv\n",
            "Chunk 5 saved to: /content/chunks/chunk_5.csv\n",
            "Chunk 6 saved to: /content/chunks/chunk_6.csv\n",
            "Chunk 7 saved to: /content/chunks/chunk_7.csv\n",
            "Chunk 8 saved to: /content/chunks/chunk_8.csv\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-5-868cbddd9b93>:62: DtypeWarning: Columns (6) have mixed types. Specify dtype option on import or set low_memory=False.\n",
            "  df = pd.read_csv('/content/3.6M-Japan-lifebear.com-Largest-Notebook-App-UsersDB-csv-2019.csv', sep=';', low_memory=True)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   id    login_id              mail_address                          password  \\\n",
            "0   1    sugimoto   sugimoto@lifebear.co.jp  f0bac04aa1b45cf443d722d6f71c0250   \n",
            "1   2         kou  nakanishi@lifebear.co.jp  48207c322ee5bb156ffec9f08c960aaa   \n",
            "2   3      yusuke     yuozawa1208@gmail.com  048261a8024ce51d379eb53cc51aaf33   \n",
            "3   4  entyan1106        endo1106@gmail.com  cd77a9dac26260a104facda5665eb3ab   \n",
            "4   5      kuriki          kuriki@wavy4.com  a026597c294cc48cd20ae361f10cbab1   \n",
            "\n",
            "            created_at          salt birthday_on  gender  \n",
            "0  2012-01-13 22:54:05  yGwBKynnsctI  1984-11-09     0.0  \n",
            "1  2012-01-14 12:48:31  aha6EuRYCDvU  1986-11-13     0.0  \n",
            "2  2012-01-17 15:33:22  PVS59dPWk9BH  1984-12-08     0.0  \n",
            "3  2012-01-17 15:37:02  vLZI6TVCJowN  1987-11-06     0.0  \n",
            "4  2012-01-17 18:52:32  swFznWWk79fg  1986-10-21     0.0  \n",
            "\n",
            "Missing values per column:\n",
            " id                    0\n",
            "login_id              2\n",
            "mail_address          0\n",
            "password              0\n",
            "created_at            0\n",
            "salt                  0\n",
            "birthday_on     3294901\n",
            "gender          1317523\n",
            "dtype: int64\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-5-868cbddd9b93>:86: DtypeWarning: Columns (6) have mixed types. Specify dtype option on import or set low_memory=False.\n",
            "  df = pd.read_csv(file_path, sep=';', low_memory=True, encoding='latin-1')  # Specify encoding if needed\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Missing values per column:\n",
            " id                    0\n",
            "login_id              2\n",
            "mail_address          0\n",
            "password              0\n",
            "created_at            0\n",
            "salt                  0\n",
            "birthday_on     3294901\n",
            "gender          1317523\n",
            "dtype: int64\n",
            "Chunk 1 saved to: /content/chunks/chunk_1.csv\n",
            "Chunk 2 saved to: /content/chunks/chunk_2.csv\n",
            "Chunk 3 saved to: /content/chunks/chunk_3.csv\n",
            "Chunk 4 saved to: /content/chunks/chunk_4.csv\n",
            "Chunk 5 saved to: /content/chunks/chunk_5.csv\n",
            "Chunk 6 saved to: /content/chunks/chunk_6.csv\n",
            "Chunk 7 saved to: /content/chunks/chunk_7.csv\n",
            "Chunk 8 saved to: /content/chunks/chunk_8.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: update this code above to find invalid emails in headers column \"mail_address\".  create a folder, name it dump and remove all invalid \"mail_address\" and place in a folder call dump and export to csv\n",
        "\n",
        "import pandas as pd\n",
        "import os\n",
        "import re\n",
        "\n",
        "# Read the CSV file with specified separator and low_memory option\n",
        "df = pd.read_csv('/content/3.6M-Japan-lifebear.com-Largest-Notebook-App-UsersDB-csv-2019.csv', sep=';', low_memory=True)\n",
        "\n",
        "# Function to check if an email is valid\n",
        "def is_valid_email(email):\n",
        "  if not isinstance(email, str):\n",
        "    return False\n",
        "  email_regex = r\"^[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\\.[a-zA-Z]{2,}$\"\n",
        "  return bool(re.match(email_regex, email))\n",
        "\n",
        "\n",
        "# Create a folder named 'dump' if it doesn't exist\n",
        "dump_folder = '/content/dump'\n",
        "if not os.path.exists(dump_folder):\n",
        "    os.makedirs(dump_folder)\n",
        "\n",
        "# Iterate through the DataFrame and find invalid email addresses in the 'mail_address' column\n",
        "invalid_emails_df = df[~df['mail_address'].apply(is_valid_email)]\n",
        "\n",
        "# Save the rows with invalid emails to a CSV file in the 'dump' folder\n",
        "invalid_emails_df.to_csv(os.path.join(dump_folder, 'invalid_emails.csv'), index=False)\n",
        "\n",
        "print(f\"Invalid email addresses saved to: {os.path.join(dump_folder, 'invalid_emails.csv')}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "19FRsrVpdM1V",
        "outputId": "de5a5524-a36f-4a11-eadf-83b1571417c7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-7-eb6a63962913>:8: DtypeWarning: Columns (6) have mixed types. Specify dtype option on import or set low_memory=False.\n",
            "  df = pd.read_csv('/content/3.6M-Japan-lifebear.com-Largest-Notebook-App-UsersDB-csv-2019.csv', sep=';', low_memory=True)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Invalid email addresses saved to: /content/dump/invalid_emails.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "```python\n",
        "import pandas as pd\n",
        "import os\n",
        "import re\n",
        "\n",
        "\n",
        "# Read the CSV file with specified separator and low_memory option\n",
        "df = pd.read_csv('/content/3.6M-Japan-lifebear.com-Largest-Notebook-App-UsersDB-csv-2019.csv', sep=';', low_memory=True)\n",
        "\n",
        "# Print the first 5 rows of the DataFrame\n",
        "print(df.head())\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def split_csv(file_path, chunk_size, output_directory):\n",
        "    \"\"\"\n",
        "    Splits a CSV file into multiple chunks based on the specified chunk size.\n",
        "\n",
        "    Args:\n",
        "        file_path (str): Path to the input CSV file.\n",
        "        chunk_size (int): Number of rows per chunk.\n",
        "        output_directory (str): Directory to save the output chunks.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        if not os.path.exists(output_directory):\n",
        "            os.makedirs(output_directory)\n",
        "\n",
        "        df = pd.read_csv(file_path, sep=';', low_memory=True, encoding='latin-1')  # Specify encoding if needed\n",
        "\n",
        "        num_chunks = (len(df) // chunk_size) + 1\n",
        "\n",
        "        for i in range(num_chunks):\n",
        "            start_row = i * chunk_size\n",
        "            end_row = min((i + 1) * chunk_size, len(df))\n",
        "            chunk = df.iloc[start_row:end_row]\n",
        "\n",
        "            output_file = os.path.join(output_directory, f\"chunk_{i+1}.csv\")\n",
        "            chunk.to_csv(output_file, index=False)\n",
        "\n",
        "            print(f\"Chunk {i+1} saved to: {output_file}\")\n",
        "\n",
        "    except FileNotFoundError:\n",
        "        print(f\"Error: File not found at {file_path}\")\n",
        "    except Exception as e:\n",
        "        print(f\"An error occurred: {e}\")\n",
        "\n",
        "\n",
        "# Example usage:\n",
        "file_path = \"/content/3.6M-Japan-lifebear.com-Largest-Notebook-App-UsersDB-csv-2019.csv\"\n",
        "chunk_size = 500000  # 1 million rows per chunk\n",
        "output_directory = \"/content/chunks\"\n",
        "\n",
        "split_csv(file_path, chunk_size, output_directory)\n",
        "\n",
        "\n",
        "\n",
        "# Read the CSV file with specified separator and low_memory option\n",
        "df = pd.read_csv('/content/3.6M-Japan-lifebear.com-Largest-Notebook-App-UsersDB-csv-2019.csv', sep=';', low_memory=True)\n",
        "\n",
        "# Print the first 5 rows of the DataFrame\n",
        "print(df.head())\n",
        "\n",
        "\n",
        "# Check for missing values in the DataFrame\n",
        "missing_values = df.isnull().sum()\n",
        "print(\"\\nMissing values per column:\\n\", missing_values)\n",
        "\n",
        "\n",
        "def split_csv(file_path, chunk_size, output_directory):\n",
        "    \"\"\"\n",
        "    Splits a CSV file into multiple chunks based on the specified chunk size.\n",
        "\n",
        "    Args:\n",
        "        file_path (str): Path to the input CSV file.\n",
        "        chunk_size (int): Number of rows per chunk.\n",
        "        output_directory (str): Directory to save the output chunks.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        if not os.path.exists(output_directory):\n",
        "            os.makedirs(output_directory)\n",
        "\n",
        "        df = pd.read_csv(file_path, sep=';', low_memory=True, encoding='latin-1')  # Specify encoding if needed\n",
        "\n",
        "        # Check for missing values in the DataFrame before splitting\n",
        "        missing_values = df.isnull().sum()\n",
        "        print(\"\\nMissing values per column:\\n\", missing_values)\n",
        "\n",
        "        num_chunks = (len(df) // chunk_size) + 1\n",
        "\n",
        "        for i in range(num_chunks):\n",
        "            start_row = i * chunk_size\n",
        "            end_row = min((i + 1) * chunk_size, len(df))\n",
        "            chunk = df.iloc[start_row:end_row]\n",
        "\n",
        "            output_file = os.path.join(output_directory, f\"chunk_{i+1}.csv\")\n",
        "            chunk.to_csv(output_file, index=False)\n",
        "\n",
        "            print(f\"Chunk {i+1} saved to: {output_file}\")\n",
        "\n",
        "    except FileNotFoundError:\n",
        "        print(f\"Error: File not found at {file_path}\")\n",
        "    except Exception as e:\n",
        "        print(f\"An error occurred: {e}\")\n",
        "\n",
        "\n",
        "# Example usage:\n",
        "import re\n",
        "\n",
        "\n",
        "# Read the CSV file with specified separator and low_memory option\n",
        "df = pd.read_csv('/content/3.6M-Japan-lifebear.com-Largest-Notebook-App-UsersDB-csv-2019.csv', sep=';', low_memory=True)\n",
        "\n",
        "# Print the first 5 rows of the DataFrame\n",
        "print(df.head())\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def split_csv(file_path, chunk_size, output_directory):\n",
        "    \"\"\"\n",
        "    Splits a CSV file into multiple chunks based on the specified chunk size.\n",
        "\n",
        "    Args:\n",
        "        file_path (str): Path to the input CSV file.\n",
        "        chunk_size (int): Number of rows per chunk.\n",
        "        output_directory (str): Directory to save the output chunks.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        if not os.path.exists(output_directory):\n",
        "            os.makedirs(output_directory)\n",
        "\n",
        "        df = pd.read_csv(file_path, sep=';', low_memory=True, encoding='latin-1')  # Specify encoding if needed\n",
        "\n",
        "        num_chunks = (len(df) // chunk_size) + 1\n",
        "\n",
        "        for i in range(num_chunks):\n",
        "            start_row = i * chunk_size\n",
        "            end_row = min((i + 1) * chunk_size, len(df))\n",
        "            chunk = df.iloc[start_row:end_row]\n",
        "\n",
        "            output_file = os.path.join(output_directory, f\"chunk_{i+1}.csv\")\n",
        "            chunk.to_csv(output_file, index=False)\n",
        "\n",
        "            print(f\"Chunk {i+1} saved to: {output_file}\")\n",
        "\n",
        "    except FileNotFoundError:\n",
        "        print(f\"Error: File not found at {file_path}\")\n",
        "    except Exception as e:\n",
        "        print(f\"An error occurred: {e}\")\n",
        "\n",
        "\n",
        "# Example usage:\n",
        "file_path = \"/content/3.6M-Japan-lifebear.com-Largest-Notebook-App-UsersDB-csv-2019.csv\"\n",
        "chunk_size = 500000  # 1 million rows per chunk\n",
        "output_directory = \"/content/chunks\"\n",
        "\n",
        "split_csv(file_path, chunk_size, output_directory)\n",
        "\n",
        "\n",
        "\n",
        "# Read the CSV file with specified separator and low_memory option\n",
        "df = pd.read_csv('/content/3.6M-Japan-lifebear.com-Largest-Notebook-App-UsersDB-csv-2019.csv', sep=';', low_memory=True)\n",
        "\n",
        "# Print the first 5 rows of the DataFrame\n",
        "print(df.head())\n",
        "\n",
        "\n",
        "# Check for missing values in the DataFrame\n",
        "missing_values = df.isnull().sum()\n",
        "print(\"\\nMissing values per column:\\n\", missing_values)\n",
        "\n",
        "\n",
        "def split_csv(file_path, chunk_size, output_directory):\n",
        "    \"\"\"\n",
        "    Splits a CSV file into multiple chunks based on the specified chunk size.\n",
        "\n",
        "    Args:\n",
        "        file_path (str): Path to the input CSV file.\n",
        "        chunk_size (int): Number of rows per chunk.\n",
        "        output_directory (str): Directory to save the output chunks.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        if not os.path.exists(output_directory):\n",
        "            os.makedirs(output_directory)\n",
        "\n",
        "        df = pd.read_csv(file_path, sep=';', low_memory=True, encoding='latin-1')  # Specify encoding if needed\n",
        "\n",
        "        # Check for missing values in the DataFrame before splitting\n",
        "        missing_values = df.isnull().sum()\n",
        "        print(\"\\nMissing values per column:\\n\", missing_values)\n",
        "\n",
        "        num_chunks = (len(df) // chunk_size) + 1\n",
        "\n",
        "        for i in range(num_chunks):\n",
        "            start_row = i * chunk_size\n",
        "            end_row = min((i + 1) * chunk_size, len(df))\n",
        "            chunk = df.iloc[start_row:end_row]\n",
        "\n",
        "            output_file = os.path.join(output_directory, f\"chunk_{i+1}.csv\")\n",
        "            chunk.to_csv(output_file, index=False)\n",
        "\n",
        "            print(f\"Chunk {i+1} saved to: {output_file}\")\n",
        "\n",
        "    except FileNotFoundError:\n",
        "        print(f\"Error: File not found at {file_path}\")\n",
        "    except Exception as e:\n",
        "        print(f\"An error occurred: {e}\")\n",
        "\n",
        "\n",
        "# Example usage:\n",
        "import re\n",
        "\n",
        "\n",
        "# Read the CSV file with specified separator and low_memory option\n",
        "df = pd.read_csv('/content/3.6M-Japan-lifebear.com-Largest-Notebook-App-UsersDB-csv-2019.csv', sep=';', low_memory=True)\n",
        "\n",
        "# Print the first 5 rows of the DataFrame\n",
        "print(df.head())\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def split_csv(file_path, chunk_size, output_directory):\n",
        "    \"\"\"\n",
        "    Splits a CSV file into multiple chunks based on the specified chunk size.\n",
        "\n",
        "    Args:\n",
        "        file_path (str): Path to the input CSV file.\n",
        "        chunk_size (int): Number of rows per chunk.\n",
        "        output_directory (str): Directory to save the output chunks.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        if not os.path.exists(output_directory):\n",
        "            os.makedirs(output_directory)\n",
        "\n",
        "        df = pd.read_csv(file_path, sep=';', low_memory=True, encoding='latin-1')  # Specify encoding if needed\n",
        "\n",
        "        num_chunks = (len(df) // chunk_size) + 1\n",
        "\n",
        "        for i in range(num_chunks):\n",
        "            start_row = i * chunk_size\n",
        "            end_row = min((i + 1) * chunk_size, len(df))\n",
        "            chunk = df.iloc[start_row:end_row]\n",
        "\n",
        "            output_file = os.path.join(output_directory, f\"chunk_{i+1}.csv\")\n",
        "            chunk.to_csv(output_file, index=False)\n",
        "\n",
        "            print(f\"Chunk {i+1} saved to: {output_file}\")\n",
        "\n",
        "    except FileNotFoundError:\n",
        "        print(f\"Error: File not found at {file_path}\")\n",
        "    except Exception as e:\n",
        "        print(f\"An error occurred: {e}\")\n",
        "\n",
        "\n",
        "# Example usage:\n",
        "file_path = \"/content/3.6M-Japan-lifebear.com-Largest-Notebook-App-UsersDB-csv-2019.csv\"\n",
        "chunk_size = 500000  # 1 million rows per chunk\n",
        "output_directory = \"/content/chunks\"\n",
        "\n",
        "split_csv(file_path, chunk_size, output_directory)\n",
        "\n",
        "\n",
        "\n",
        "# Read the CSV file with specified separator and low_memory option\n",
        "df = pd.read_csv('/content/3.6M-Japan-lifebear.com-Largest-Notebook-App-UsersDB-csv-2019.csv', sep=';', low_memory=True)\n",
        "\n",
        "# Print the first 5 rows of the DataFrame\n",
        "print(df.head())\n",
        "\n",
        "\n",
        "# Check for missing values in the DataFrame\n",
        "missing_values = df.isnull().sum()\n",
        "print(\"\\nMissing values per column:\\n\", missing_values)\n",
        "\n",
        "\n",
        "def split_csv(file_path, chunk_size, output_directory):\n",
        "    \"\"\"\n",
        "    Splits a CSV file into multiple chunks based on the specified chunk size.\n",
        "\n",
        "    Args:\n",
        "        file_path (str): Path to the input CSV file.\n",
        "        chunk_size (int): Number of rows per chunk.\n",
        "        output_directory (str): Directory to save the output chunks.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        if not os.path.exists(output_directory):\n",
        "            os.makedirs(output_directory)\n",
        "\n",
        "        df = pd.read_csv(file_path, sep=';', low_memory=True, encoding='latin-1')  # Specify encoding if needed\n",
        "\n",
        "        # Check for missing values in the DataFrame before splitting\n",
        "        missing_values = df.isnull().sum()\n",
        "        print(\"\\nMissing values per column:\\n\", missing_values)\n",
        "\n",
        "        num_chunks = (len(df) // chunk_size) + 1\n",
        "\n",
        "        for i in range(num_chunks):\n",
        "            start_row = i * chunk_size\n",
        "            end_row = min((i + 1) * chunk_size, len(df))\n",
        "            chunk = df.iloc[start_row:end_row]\n",
        "\n",
        "            output_file = os.path.join(output_directory, f\"chunk_{i+1}.csv\")\n",
        "            chunk.to_csv(output_file, index=False)\n",
        "\n",
        "            print(f\"Chunk {i+1} saved to: {output_file}\")\n",
        "\n",
        "    except FileNotFoundError:\n",
        "        print(f\"Error: File not found at {file_path}\")\n",
        "    except Exception as e:\n",
        "        print(f\"An error occurred: {e}\")\n",
        "\n",
        "\n",
        "# Example usage:\n",
        "import re\n",
        "\n",
        "\n",
        "# Read the CSV file with specified separator and low_memory option\n",
        "df = pd.read_csv('/content/3.6M-Japan-lifebear.com-Largest-Notebook-App-UsersDB-csv-2019.csv', sep=';', low_memory=True)\n",
        "\n",
        "# Print the first 5 rows of the DataFrame\n",
        "print(df.head())\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def split_csv(file_path, chunk_size, output_directory):\n",
        "    \"\"\"\n",
        "    Splits a CSV file into multiple chunks based on the specified chunk size.\n",
        "\n",
        "    Args:\n",
        "        file_path (str): Path to the input CSV file.\n",
        "        chunk_size (int): Number of rows per chunk.\n",
        "        output_directory (str): Directory to save the output chunks.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        if not os.path.exists(output_directory):\n",
        "            os.makedirs(output_directory)\n",
        "\n",
        "        df = pd.read_csv(file_path, sep=';', low_memory=True, encoding='latin-1')  # Specify encoding if needed\n",
        "\n",
        "        num_chunks = (len(df) // chunk_size) + 1\n",
        "\n",
        "        for i in range(num_chunks):\n",
        "            start_row = i * chunk_size\n",
        "            end_row = min((i + 1) * chunk_size, len(df))\n",
        "            chunk = df.iloc[start_row:end_row]\n",
        "\n",
        "            output_file = os.path.join(output_directory, f\"chunk_{i+1}.csv\")\n",
        "            chunk.to_csv(output_file, index=False)\n",
        "\n",
        "            print(f\"Chunk {i+1} saved to: {output_file}\")\n",
        "\n",
        "    except FileNotFoundError:\n",
        "        print(f\"Error: File not found at {file_path}\")\n",
        "    except Exception as e:\n",
        "        print(f\"An error occurred: {e}\")\n",
        "\n",
        "\n",
        "# Example usage:\n",
        "file_path = \"/content/3.6M-Japan-lifebear.com-Largest-Notebook-App-UsersDB-csv-2019.csv\"\n",
        "chunk_size = 500000  # 1 million rows per chunk\n",
        "output_directory = \"/content/chunks\"\n",
        "\n",
        "split_csv(file_path, chunk_size, output_directory)\n",
        "\n",
        "\n",
        "\n",
        "# Read the CSV file with specified separator and low_memory option\n",
        "df = pd.read_csv('/content/3.6M-Japan-lifebear.com-Largest-Notebook-App-UsersDB-csv-2019.csv', sep=';', low_memory=True)\n",
        "\n",
        "# Print the first 5 rows of the DataFrame\n",
        "print(df.head())\n",
        "\n",
        "\n",
        "# Check for missing values in the DataFrame\n",
        "missing_values = df.isnull().sum()\n",
        "print(\"\\nMissing values per column:\\n\", missing_values)\n",
        "\n",
        "\n",
        "def split_csv(file_path, chunk_size, output_directory):\n",
        "    \"\"\"\n",
        "    Splits a CSV file into multiple chunks based on the specified chunk size.\n",
        "\n",
        "    Args:\n",
        "        file_path (str): Path to the input CSV file.\n",
        "        chunk_size (int): Number of rows per chunk.\n",
        "        output_directory (str): Directory to save the output chunks.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        if not os.path.exists(output_directory):\n",
        "            os.makedirs(output_directory)\n",
        "\n",
        "        df = pd.read_csv(file_path, sep=';', low_memory=True, encoding='latin-1')  # Specify encoding if needed\n",
        "\n",
        "        # Check for missing values in the DataFrame before splitting\n",
        "        missing_values = df.isnull().sum()\n",
        "        print(\"\\nMissing values per column:\\n\", missing_values)\n",
        "\n",
        "        num_chunks = (len(df) // chunk_size) + 1\n",
        "\n",
        "        for i in range(num_chunks):\n",
        "            start_row = i * chunk_size\n",
        "            end_row = min((i + 1) * chunk_size, len(df))\n",
        "            chunk = df.iloc[start_row:end_row]\n",
        "\n",
        "            output_file = os.path.join(output_directory, f\"chunk_{i+1}.csv\")\n",
        "            chunk.to_csv(output_file, index=False)\n",
        "\n",
        "            print(f\"Chunk {i+1} saved to: {output_file}\")\n",
        "\n",
        "    except FileNotFoundError:\n",
        "        print(f\"Error: File not found at {file_path}\")\n",
        "    except Exception as e:\n",
        "        print(f\"An error occurred: {e}\")\n",
        "\n",
        "\n",
        "# Example usage:\n",
        "import re\n",
        "\n",
        "\n",
        "# Read the CSV file with specified separator and low_memory option\n",
        "df = pd.read_csv('/content/3.6M-Japan-lifebear.com-Largest-Notebook-App-UsersDB-csv-2019.csv', sep=';', low_memory=True)\n",
        "\n",
        "# Print the first 5 rows of the DataFrame\n",
        "print(df.head())\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def split_csv(file_path, chunk_size, output_directory):\n",
        "    \"\"\"\n",
        "    Splits a CSV file into multiple chunks based on the specified chunk size.\n",
        "\n",
        "    Args:\n",
        "        file_path (str): Path to the input CSV file.\n",
        "        chunk_size (int): Number of rows per chunk.\n",
        "        output_directory (str): Directory to save the output chunks.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        if not os.path.exists(output_directory):\n",
        "            os.makedirs(output_directory)\n",
        "\n",
        "        df = pd.read_csv(file_path, sep=';', low_memory=True, encoding='latin-1')  # Specify encoding if needed\n",
        "\n",
        "        num_chunks = (len(df) // chunk_size) + 1\n",
        "\n",
        "        for i in range(num_chunks):\n",
        "            start_row = i * chunk_size\n",
        "            end_row = min((i + 1) * chunk_size, len(df))\n",
        "            chunk = df.iloc[start_row:end_row]\n",
        "\n",
        "            output_file = os.path.join(output_directory, f\"chunk_{i+1}.csv\")\n",
        "            chunk.to_csv(output_file, index=False)\n",
        "\n",
        "            print(f\"Chunk {i+1} saved to: {output_file}\")\n",
        "\n",
        "    except FileNotFoundError:\n",
        "        print(f\"Error: File not found at {file_path}\")\n",
        "    except Exception as e:\n",
        "        print(f\"An error occurred: {e}\")\n",
        "\n",
        "\n",
        "# Example usage:\n",
        "file_path = \"/content/3.6M-Japan-lifebear.com-Largest-Notebook-App-UsersDB-csv-2019.csv\"\n",
        "chunk_size = 500000  # 1 million rows per chunk\n",
        "output_directory = \"/content/chunks\"\n",
        "\n",
        "split_csv(file_path, chunk_size, output_directory)\n",
        "\n",
        "\n",
        "\n",
        "# Read the CSV file with specified separator and low_memory option\n",
        "df = pd.read_csv('/content/3.6M-Japan-lifebear.com-Largest-Notebook-App-UsersDB-csv-2019.csv', sep=';', low_memory=True)\n",
        "\n",
        "# Print the first 5 rows of the DataFrame\n",
        "print(df.head())\n",
        "\n",
        "\n",
        "# Check for missing values in the DataFrame\n",
        "missing_values = df.isnull().sum()\n",
        "print(\"\\nMissing values per column:\\n\", missing_values)\n",
        "\n",
        "\n",
        "def split_csv(file_path, chunk_size, output_directory):\n",
        "    \"\"\"\n",
        "    Splits a CSV file into multiple chunks based on the specified chunk size.\n",
        "\n",
        "    Args:\n",
        "        file_path (str): Path to the input CSV file.\n",
        "        chunk_size (int): Number of rows per chunk.\n",
        "        output_directory (str): Directory to save the output chunks.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        if not os.path.exists(output_directory):\n",
        "            os.makedirs(output_directory)\n",
        "\n",
        "        df = pd.read_csv(file_path, sep=';', low_memory=True, encoding='latin-1')  # Specify encoding if needed\n",
        "\n",
        "        # Check for missing values in the DataFrame before splitting\n",
        "        missing_values = df.isnull().sum()\n",
        "        print(\"\\nMissing values per column:\\n\", missing_values)\n",
        "\n",
        "        num_chunks = (len(df) // chunk_size) + 1\n",
        "\n",
        "        for i in range(num_chunks):\n",
        "            start_row = i * chunk_size\n",
        "            end_row = min((i + 1) * chunk_size, len(df))\n",
        "            chunk = df.iloc[start_row:end_row]\n",
        "\n",
        "            output_file = os.path.join(output_directory, f\"chunk_{i+1}.csv\")\n",
        "            chunk.to_csv(output_file, index=False)\n",
        "\n",
        "            print(f\"Chunk {i+1} saved to: {output_file}\")\n",
        "\n",
        "    except FileNotFoundError:\n",
        "        print(f\"Error: File not found at {file_path}\")\n",
        "    except Exception as e:\n",
        "        print(f\"An error occurred: {e}\")\n",
        "\n",
        "\n",
        "# Example usage:\n",
        "file_path = \"/content/3.6M-Japan-lifebear.com-Largest-Notebook-App-UsersDB-csv-2019.csv\"\n",
        "chunk_size = 500000  # 1 million rows per chunk\n",
        "output_directory = \"/content/chunks\"\n",
        "\n",
        "split_csv(file_path, chunk_size, output_directory)\n",
        "\n",
        "\n",
        "\n",
        "# Read the CSV file with specified separator and low_memory option\n",
        "df = pd.read_csv('/content/3.6M-Japan-lifebear.com-Largest-Notebook-App-UsersDB-csv-2019.csv', sep=';', low_memory=True)\n",
        "\n",
        "# Print the first 5 rows of the DataFrame\n",
        "print(df.head())\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def split_csv(file_path, chunk_size, output_directory):\n",
        "    \"\"\"\n",
        "    Splits a CSV file into multiple chunks based on the specified chunk size.\n",
        "\n",
        "    Args:\n",
        "        file_path (str): Path to the input CSV file.\n",
        "        chunk_size (int): Number of rows per chunk.\n",
        "        output_directory (str): Directory to save the output chunks.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        if not os.path.exists(output_directory):\n",
        "            os.makedirs(output_directory)\n",
        "\n",
        "        df = pd.read_csv(file_path, sep=';', low_memory=True, encoding='latin-1')  # Specify encoding if needed\n",
        "\n",
        "        # Remove duplicate rows based on all columns\n",
        "        df = df.drop_duplicates()\n",
        "\n",
        "        num_chunks = (len(df) // chunk_size) + 1\n",
        "\n",
        "        for i in range(num_chunks):\n",
        "            start_row = i * chunk_size\n",
        "            end_row = min((i + 1) * chunk_size, len(df))\n",
        "            chunk = df.iloc[start_row:end_row]\n",
        "\n",
        "            output_file = os.path.join(output_directory, f\"chunk_{i+1}.csv\")\n",
        "            chunk.to_csv(output_file, index=False)\n",
        "\n",
        "            print(f\"Chunk {i+1} saved to: {output_file}\")\n",
        "\n",
        "    except FileNotFoundError:\n",
        "        print(f\"Error: File not found at {file_path}\")\n",
        "    except Exception as e:\n",
        "        print(f\"An error occurred: {e}\")\n",
        "\n",
        "\n",
        "# Example usage:\n",
        "file_path = \"/content/3.6M-Japan-lifebear.com-Largest-Notebook-App-UsersDB-csv-2019.csv\"\n",
        "chunk_size = 500000  # 1 million rows per chunk\n",
        "output_directory = \"/content/chunks\"\n",
        "\n",
        "split_csv(file_path, chunk_size, output_directory)\n",
        "\n",
        "\n",
        "\n",
        "# Read the CSV file with specified separator and low_memory option\n",
        "df = pd.read_csv('/content/3.6M-Japan-lifebear.com-Largest-Notebook-App-UsersDB-csv-2019.csv', sep=';', low_memory=True)\n",
        "\n",
        "# Print the first 5 rows of the DataFrame\n",
        "print(df.head())\n",
        "\n",
        "\n",
        "# Check for missing values in the DataFrame\n",
        "missing_values = df.isnull().sum()\n",
        "print(\"\\nMissing values per column:\\n\", missing_values)\n",
        "\n",
        "\n",
        "def split_csv(file_path, chunk_size, output_directory):\n",
        "    \"\"\"\n",
        "    Splits a CSV file into multiple chunks based on the specified chunk size.\n",
        "\n",
        "    Args:\n",
        "        file_path (str): Path to the input CSV file.\n",
        "        chunk_size (int): Number of rows per chunk.\n",
        "        output_directory (str): Directory to save the output chunks.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        if not os.path.exists(output_directory):\n",
        "            os.makedirs(output_directory)\n",
        "\n",
        "        df = pd.read_csv(file_path, sep=';', low_memory=True, encoding='latin-1')  # Specify encoding if needed\n",
        "\n",
        "        # Remove duplicate rows based on all columns\n",
        "        df = df.drop_duplicates()\n",
        "\n",
        "        # Check for missing values in the DataFrame before splitting\n",
        "        missing_values = df.isnull().sum()\n",
        "        print(\"\\nMissing values per column:\\n\", missing_values)\n",
        "\n",
        "        num_chunks = (len(df) // chunk_size) + 1\n",
        "\n",
        "        for i in range(num_chunks):\n",
        "            start_row = i * chunk_size\n",
        "            end_row = min((i + 1) * chunk_size, len(df))\n",
        "            chunk = df.iloc[start_row:end_row]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 106
        },
        "id": "tucEj0-MgYFJ",
        "outputId": "81aa58dc-f5a7-4cf0-b561-2a24e8fbd76e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "invalid syntax (<ipython-input-12-f513ec15c481>, line 1)",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-12-f513ec15c481>\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    ```python\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import os\n",
        "import re\n",
        "\n",
        "def split_csv(file_path, chunk_size, output_directory):\n",
        "    \"\"\"\n",
        "    Splits a CSV file into multiple chunks based on the specified chunk size.\n",
        "\n",
        "    Args:\n",
        "        file_path (str): Path to the input CSV file.\n",
        "        chunk_size (int): Number of rows per chunk.\n",
        "        output_directory (str): Directory to save the output chunks.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        if not os.path.exists(output_directory):\n",
        "            os.makedirs(output_directory)\n",
        "\n",
        "        df = pd.read_csv(file_path, sep=';', low_memory=True, encoding='latin-1')  # Specify encoding if needed\n",
        "\n",
        "        # Remove duplicate rows based on all columns\n",
        "        df = df.drop_duplicates()\n",
        "\n",
        "        # Check for missing values in the DataFrame before splitting\n",
        "        missing_values = df.isnull().sum()\n",
        "        print(\"\\nMissing values per column:\\n\", missing_values)\n",
        "\n",
        "        num_chunks = (len(df) // chunk_size) + 1\n",
        "\n",
        "        for i in range(num_chunks):\n",
        "            start_row = i * chunk_size\n",
        "            end_row = min((i + 1) * chunk_size, len(df))\n",
        "            chunk = df.iloc[start_row:end_row]\n",
        "\n",
        "            output_file = os.path.join(output_directory, f\"chunk_{i+1}.csv\")\n",
        "            chunk.to_csv(output_file, index=False)\n",
        "\n",
        "            print(f\"Chunk {i+1} saved to: {output_file}\")\n",
        "\n",
        "    except FileNotFoundError:\n",
        "        print(f\"Error: File not found at {file_path}\")\n",
        "    except Exception as e:\n",
        "        print(f\"An error occurred: {e}\")\n",
        "\n",
        "\n",
        "# Example usage:\n",
        "file_path = \"/content/3.6M-Japan-lifebear.com-Largest-Notebook-App-UsersDB-csv-2019.csv\"\n",
        "chunk_size = 500000  # 1 million rows per chunk\n",
        "output_directory = \"/content/chunks\"\n",
        "\n",
        "split_csv(file_path, chunk_size, output_directory)\n",
        "\n",
        "\n",
        "\n",
        "# Read the CSV file with specified separator and low_memory option\n",
        "df = pd.read_csv('/content/3.6M-Japan-lifebear.com-Largest-Notebook-App-UsersDB-csv-2019.csv', sep=';', low_memory=True)\n",
        "\n",
        "# Print the first 5 rows of the DataFrame\n",
        "print(df.head())\n",
        "\n",
        "\n",
        "# Check for missing values in the DataFrame\n",
        "missing_values = df.isnull().sum()\n",
        "print(\"\\nMissing values per column:\\n\", missing_values)\n",
        "\n",
        "# Function to check if an email is valid\n",
        "def is_valid_email(email):\n",
        "    if not isinstance(email, str):\n",
        "        return False\n",
        "    email_regex = r\"^[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\\.[a-zA-Z]{2,}$\"\n",
        "    return bool(re.match(email_regex, email))\n",
        "\n",
        "\n",
        "# Create a folder named 'dump' if it doesn't exist\n",
        "dump_folder = '/content/dump'\n",
        "if not os.path.exists(dump_folder):\n",
        "    os.makedirs(dump_folder)\n",
        "\n",
        "# Iterate through the DataFrame and find invalid email addresses in the 'mail_address' column\n",
        "invalid_emails_df = df[~df['mail_address'].apply(is_valid_email)]\n",
        "\n",
        "# Save the rows with invalid emails to a CSV file in the 'dump' folder\n",
        "invalid_emails_df.to_csv(os.path.join(dump_folder, 'invalid_emails.csv'), index=False)\n",
        "\n",
        "print(f\"Invalid email addresses saved to: {os.path.join(dump_folder, 'invalid_emails.csv')}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BQg0Xx00iQCF",
        "outputId": "c485bb19-bf6d-4bea-abe7-e0fa88d6cfc7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-13-1ede29ef5032>:18: DtypeWarning: Columns (6) have mixed types. Specify dtype option on import or set low_memory=False.\n",
            "  df = pd.read_csv(file_path, sep=';', low_memory=True, encoding='latin-1')  # Specify encoding if needed\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Missing values per column:\n",
            " id                    0\n",
            "login_id              2\n",
            "mail_address          0\n",
            "password              0\n",
            "created_at            0\n",
            "salt                  0\n",
            "birthday_on     3294901\n",
            "gender          1317523\n",
            "dtype: int64\n",
            "Chunk 1 saved to: /content/chunks/chunk_1.csv\n",
            "Chunk 2 saved to: /content/chunks/chunk_2.csv\n",
            "Chunk 3 saved to: /content/chunks/chunk_3.csv\n",
            "Chunk 4 saved to: /content/chunks/chunk_4.csv\n",
            "Chunk 5 saved to: /content/chunks/chunk_5.csv\n",
            "Chunk 6 saved to: /content/chunks/chunk_6.csv\n",
            "Chunk 7 saved to: /content/chunks/chunk_7.csv\n",
            "Chunk 8 saved to: /content/chunks/chunk_8.csv\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-13-1ede29ef5032>:55: DtypeWarning: Columns (6) have mixed types. Specify dtype option on import or set low_memory=False.\n",
            "  df = pd.read_csv('/content/3.6M-Japan-lifebear.com-Largest-Notebook-App-UsersDB-csv-2019.csv', sep=';', low_memory=True)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   id    login_id              mail_address                          password  \\\n",
            "0   1    sugimoto   sugimoto@lifebear.co.jp  f0bac04aa1b45cf443d722d6f71c0250   \n",
            "1   2         kou  nakanishi@lifebear.co.jp  48207c322ee5bb156ffec9f08c960aaa   \n",
            "2   3      yusuke     yuozawa1208@gmail.com  048261a8024ce51d379eb53cc51aaf33   \n",
            "3   4  entyan1106        endo1106@gmail.com  cd77a9dac26260a104facda5665eb3ab   \n",
            "4   5      kuriki          kuriki@wavy4.com  a026597c294cc48cd20ae361f10cbab1   \n",
            "\n",
            "            created_at          salt birthday_on  gender  \n",
            "0  2012-01-13 22:54:05  yGwBKynnsctI  1984-11-09     0.0  \n",
            "1  2012-01-14 12:48:31  aha6EuRYCDvU  1986-11-13     0.0  \n",
            "2  2012-01-17 15:33:22  PVS59dPWk9BH  1984-12-08     0.0  \n",
            "3  2012-01-17 15:37:02  vLZI6TVCJowN  1987-11-06     0.0  \n",
            "4  2012-01-17 18:52:32  swFznWWk79fg  1986-10-21     0.0  \n",
            "\n",
            "Missing values per column:\n",
            " id                    0\n",
            "login_id              2\n",
            "mail_address          0\n",
            "password              0\n",
            "created_at            0\n",
            "salt                  0\n",
            "birthday_on     3294901\n",
            "gender          1317523\n",
            "dtype: int64\n",
            "Invalid email addresses saved to: /content/dump/invalid_emails.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: I have a Data Frame with header names in lowercase. convert the first letter of each header to uppercase and update this code\n",
        "\n",
        "import pandas as pd\n",
        "import os\n",
        "import re\n",
        "\n",
        "def split_csv(file_path, chunk_size, output_directory):\n",
        "    \"\"\"\n",
        "    Splits a CSV file into multiple chunks based on the specified chunk size.\n",
        "\n",
        "    Args:\n",
        "        file_path (str): Path to the input CSV file.\n",
        "        chunk_size (int): Number of rows per chunk.\n",
        "        output_directory (str): Directory to save the output chunks.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        if not os.path.exists(output_directory):\n",
        "            os.makedirs(output_directory)\n",
        "\n",
        "        df = pd.read_csv(file_path, sep=';', low_memory=True, encoding='latin-1')  # Specify encoding if needed\n",
        "\n",
        "        # Remove duplicate rows based on all columns\n",
        "        df = df.drop_duplicates()\n",
        "\n",
        "        # Check for missing values in the DataFrame before splitting\n",
        "        missing_values = df.isnull().sum()\n",
        "        print(\"\\nMissing values per column:\\n\", missing_values)\n",
        "\n",
        "        # Convert column names to title case (first letter uppercase)\n",
        "        df.rename(columns=lambda x: x.title(), inplace=True)\n",
        "\n",
        "        num_chunks = (len(df) // chunk_size) + 1\n",
        "\n",
        "        for i in range(num_chunks):\n",
        "            start_row = i * chunk_size\n",
        "            end_row = min((i + 1) * chunk_size, len(df))\n",
        "            chunk = df.iloc[start_row:end_row]\n",
        "\n",
        "            output_file = os.path.join(output_directory, f\"chunk_{i+1}.csv\")\n",
        "            chunk.to_csv(output_file, index=False)\n",
        "\n",
        "            print(f\"Chunk {i+1} saved to: {output_file}\")\n",
        "\n",
        "    except FileNotFoundError:\n",
        "        print(f\"Error: File not found at {file_path}\")\n",
        "    except Exception as e:\n",
        "        print(f\"An error occurred: {e}\")\n",
        "\n",
        "\n",
        "# Example usage:\n",
        "file_path = \"/content/3.6M-Japan-lifebear.com-Largest-Notebook-App-UsersDB-csv-2019.csv\"\n",
        "chunk_size = 500000  # 1 million rows per chunk\n",
        "output_directory = \"/content/chunks\"\n",
        "\n",
        "split_csv(file_path, chunk_size, output_directory)\n",
        "\n",
        "\n",
        "\n",
        "# Read the CSV file with specified separator and low_memory option\n",
        "df = pd.read_csv('/content/3.6M-Japan-lifebear.com-Largest-Notebook-App-UsersDB-csv-2019.csv', sep=';', low_memory=True)\n",
        "\n",
        "# Print the first 5 rows of the DataFrame\n",
        "print(df.head())\n",
        "\n",
        "\n",
        "# Check for missing values in the DataFrame\n",
        "missing_values = df.isnull().sum()\n",
        "print(\"\\nMissing values per column:\\n\", missing_values)\n",
        "\n",
        "# Function to check if an email is valid\n",
        "def is_valid_email(email):\n",
        "    if not isinstance(email, str):\n",
        "        return False\n",
        "    email_regex = r\"^[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\\.[a-zA-Z]{2,}$\"\n",
        "    return bool(re.match(email_regex, email))\n",
        "\n",
        "\n",
        "# Create a folder named 'dump' if it doesn't exist\n",
        "dump_folder = '/content/dump'\n",
        "if not os.path.exists(dump_folder):\n",
        "    os.makedirs(dump_folder)\n",
        "\n",
        "# Iterate through the DataFrame and find invalid email addresses in the 'mail_address' column\n",
        "invalid_emails_df = df[~df['mail_address'].apply(is_valid_email)]\n",
        "\n",
        "# Save the rows with invalid emails to a CSV file in the 'dump' folder\n",
        "invalid_emails_df.to_csv(os.path.join(dump_folder, 'invalid_emails.csv'), index=False)\n",
        "\n",
        "print(f\"Invalid email addresses saved to: {os.path.join(dump_folder, 'invalid_emails.csv')}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ErB3_yKljK2F",
        "outputId": "45d45040-11dd-425f-fc69-9a81cb0e557a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-14-8d49129de468>:20: DtypeWarning: Columns (6) have mixed types. Specify dtype option on import or set low_memory=False.\n",
            "  df = pd.read_csv(file_path, sep=';', low_memory=True, encoding='latin-1')  # Specify encoding if needed\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Missing values per column:\n",
            " id                    0\n",
            "login_id              2\n",
            "mail_address          0\n",
            "password              0\n",
            "created_at            0\n",
            "salt                  0\n",
            "birthday_on     3294901\n",
            "gender          1317523\n",
            "dtype: int64\n",
            "Chunk 1 saved to: /content/chunks/chunk_1.csv\n",
            "Chunk 2 saved to: /content/chunks/chunk_2.csv\n",
            "Chunk 3 saved to: /content/chunks/chunk_3.csv\n",
            "Chunk 4 saved to: /content/chunks/chunk_4.csv\n",
            "Chunk 5 saved to: /content/chunks/chunk_5.csv\n",
            "Chunk 6 saved to: /content/chunks/chunk_6.csv\n",
            "Chunk 7 saved to: /content/chunks/chunk_7.csv\n",
            "Chunk 8 saved to: /content/chunks/chunk_8.csv\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-14-8d49129de468>:60: DtypeWarning: Columns (6) have mixed types. Specify dtype option on import or set low_memory=False.\n",
            "  df = pd.read_csv('/content/3.6M-Japan-lifebear.com-Largest-Notebook-App-UsersDB-csv-2019.csv', sep=';', low_memory=True)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   id    login_id              mail_address                          password  \\\n",
            "0   1    sugimoto   sugimoto@lifebear.co.jp  f0bac04aa1b45cf443d722d6f71c0250   \n",
            "1   2         kou  nakanishi@lifebear.co.jp  48207c322ee5bb156ffec9f08c960aaa   \n",
            "2   3      yusuke     yuozawa1208@gmail.com  048261a8024ce51d379eb53cc51aaf33   \n",
            "3   4  entyan1106        endo1106@gmail.com  cd77a9dac26260a104facda5665eb3ab   \n",
            "4   5      kuriki          kuriki@wavy4.com  a026597c294cc48cd20ae361f10cbab1   \n",
            "\n",
            "            created_at          salt birthday_on  gender  \n",
            "0  2012-01-13 22:54:05  yGwBKynnsctI  1984-11-09     0.0  \n",
            "1  2012-01-14 12:48:31  aha6EuRYCDvU  1986-11-13     0.0  \n",
            "2  2012-01-17 15:33:22  PVS59dPWk9BH  1984-12-08     0.0  \n",
            "3  2012-01-17 15:37:02  vLZI6TVCJowN  1987-11-06     0.0  \n",
            "4  2012-01-17 18:52:32  swFznWWk79fg  1986-10-21     0.0  \n",
            "\n",
            "Missing values per column:\n",
            " id                    0\n",
            "login_id              2\n",
            "mail_address          0\n",
            "password              0\n",
            "created_at            0\n",
            "salt                  0\n",
            "birthday_on     3294901\n",
            "gender          1317523\n",
            "dtype: int64\n",
            "Invalid email addresses saved to: /content/dump/invalid_emails.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: I have a DataFrame with a header named 'mail_address'. rename this header to 'Email Address' and update this code\n",
        "\n",
        "import pandas as pd\n",
        "import os\n",
        "import re\n",
        "\n",
        "def split_csv(file_path, chunk_size, output_directory):\n",
        "    \"\"\"\n",
        "    Splits a CSV file into multiple chunks based on the specified chunk size.\n",
        "\n",
        "    Args:\n",
        "        file_path (str): Path to the input CSV file.\n",
        "        chunk_size (int): Number of rows per chunk.\n",
        "        output_directory (str): Directory to save the output chunks.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        if not os.path.exists(output_directory):\n",
        "            os.makedirs(output_directory)\n",
        "\n",
        "        df = pd.read_csv(file_path, sep=';', low_memory=True, encoding='latin-1')  # Specify encoding if needed\n",
        "\n",
        "        # Remove duplicate rows based on all columns\n",
        "        df = df.drop_duplicates()\n",
        "\n",
        "        # Check for missing values in the DataFrame before splitting\n",
        "        missing_values = df.isnull().sum()\n",
        "        print(\"\\nMissing values per column:\\n\", missing_values)\n",
        "\n",
        "        # Rename the 'mail_address' column to 'Email Address'\n",
        "        if 'mail_address' in df.columns:\n",
        "            df.rename(columns={'mail_address': 'Email Address'}, inplace=True)\n",
        "\n",
        "        num_chunks = (len(df) // chunk_size) + 1\n",
        "\n",
        "        for i in range(num_chunks):\n",
        "            start_row = i * chunk_size\n",
        "            end_row = min((i + 1) * chunk_size, len(df))\n",
        "            chunk = df.iloc[start_row:end_row]\n",
        "\n",
        "            output_file = os.path.join(output_directory, f\"chunk_{i+1}.csv\")\n",
        "            chunk.to_csv(output_file, index=False)\n",
        "\n",
        "            print(f\"Chunk {i+1} saved to: {output_file}\")\n",
        "\n",
        "    except FileNotFoundError:\n",
        "        print(f\"Error: File not found at {file_path}\")\n",
        "    except Exception as e:\n",
        "        print(f\"An error occurred: {e}\")\n",
        "\n",
        "\n",
        "# Example usage:\n",
        "file_path = \"/content/3.6M-Japan-lifebear.com-Largest-Notebook-App-UsersDB-csv-2019.csv\"\n",
        "chunk_size = 500000  # 1 million rows per chunk\n",
        "output_directory = \"/content/chunks\"\n",
        "\n",
        "split_csv(file_path, chunk_size, output_directory)\n",
        "\n",
        "\n",
        "\n",
        "# Read the CSV file with specified separator and low_memory option\n",
        "df = pd.read_csv('/content/3.6M-Japan-lifebear.com-Largest-Notebook-App-UsersDB-csv-2019.csv', sep=';', low_memory=True)\n",
        "\n",
        "# Print the first 5 rows of the DataFrame\n",
        "print(df.head())\n",
        "\n",
        "\n",
        "# Check for missing values in the DataFrame\n",
        "missing_values = df.isnull().sum()\n",
        "print(\"\\nMissing values per column:\\n\", missing_values)\n",
        "\n",
        "# Function to check if an email is valid\n",
        "def is_valid_email(email):\n",
        "    if not isinstance(email, str):\n",
        "        return False\n",
        "    email_regex = r\"^[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\\.[a-zA-Z]{2,}$\"\n",
        "    return bool(re.match(email_regex, email))\n",
        "\n",
        "\n",
        "# Create a folder named 'dump' if it doesn't exist\n",
        "dump_folder = '/content/dump'\n",
        "if not os.path.exists(dump_folder):\n",
        "    os.makedirs(dump_folder)\n",
        "\n",
        "# Iterate through the DataFrame and find invalid email addresses in the 'mail_address' column\n",
        "# Rename 'mail_address' to 'Email Address' if it exists\n",
        "if 'mail_address' in df.columns:\n",
        "  df.rename(columns={'mail_address': 'Email Address'}, inplace=True)\n",
        "\n",
        "# Use 'Email Address' for finding invalid emails\n",
        "if 'Email Address' in df.columns:\n",
        "  invalid_emails_df = df[~df['Email Address'].apply(is_valid_email)]\n",
        "\n",
        "  # Save the rows with invalid emails to a CSV file in the 'dump' folder\n",
        "  invalid_emails_df.to_csv(os.path.join(dump_folder, 'invalid_emails.csv'), index=False)\n",
        "\n",
        "  print(f\"Invalid email addresses saved to: {os.path.join(dump_folder, 'invalid_emails.csv')}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ycFUMQfAkTLf",
        "outputId": "4942c884-4762-4213-ad13-eaa423421016"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-15-0bf88ea4d3f4>:20: DtypeWarning: Columns (6) have mixed types. Specify dtype option on import or set low_memory=False.\n",
            "  df = pd.read_csv(file_path, sep=';', low_memory=True, encoding='latin-1')  # Specify encoding if needed\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Missing values per column:\n",
            " id                    0\n",
            "login_id              2\n",
            "mail_address          0\n",
            "password              0\n",
            "created_at            0\n",
            "salt                  0\n",
            "birthday_on     3294901\n",
            "gender          1317523\n",
            "dtype: int64\n",
            "Chunk 1 saved to: /content/chunks/chunk_1.csv\n",
            "Chunk 2 saved to: /content/chunks/chunk_2.csv\n",
            "Chunk 3 saved to: /content/chunks/chunk_3.csv\n",
            "Chunk 4 saved to: /content/chunks/chunk_4.csv\n",
            "Chunk 5 saved to: /content/chunks/chunk_5.csv\n",
            "Chunk 6 saved to: /content/chunks/chunk_6.csv\n",
            "Chunk 7 saved to: /content/chunks/chunk_7.csv\n",
            "Chunk 8 saved to: /content/chunks/chunk_8.csv\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-15-0bf88ea4d3f4>:61: DtypeWarning: Columns (6) have mixed types. Specify dtype option on import or set low_memory=False.\n",
            "  df = pd.read_csv('/content/3.6M-Japan-lifebear.com-Largest-Notebook-App-UsersDB-csv-2019.csv', sep=';', low_memory=True)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   id    login_id              mail_address                          password  \\\n",
            "0   1    sugimoto   sugimoto@lifebear.co.jp  f0bac04aa1b45cf443d722d6f71c0250   \n",
            "1   2         kou  nakanishi@lifebear.co.jp  48207c322ee5bb156ffec9f08c960aaa   \n",
            "2   3      yusuke     yuozawa1208@gmail.com  048261a8024ce51d379eb53cc51aaf33   \n",
            "3   4  entyan1106        endo1106@gmail.com  cd77a9dac26260a104facda5665eb3ab   \n",
            "4   5      kuriki          kuriki@wavy4.com  a026597c294cc48cd20ae361f10cbab1   \n",
            "\n",
            "            created_at          salt birthday_on  gender  \n",
            "0  2012-01-13 22:54:05  yGwBKynnsctI  1984-11-09     0.0  \n",
            "1  2012-01-14 12:48:31  aha6EuRYCDvU  1986-11-13     0.0  \n",
            "2  2012-01-17 15:33:22  PVS59dPWk9BH  1984-12-08     0.0  \n",
            "3  2012-01-17 15:37:02  vLZI6TVCJowN  1987-11-06     0.0  \n",
            "4  2012-01-17 18:52:32  swFznWWk79fg  1986-10-21     0.0  \n",
            "\n",
            "Missing values per column:\n",
            " id                    0\n",
            "login_id              2\n",
            "mail_address          0\n",
            "password              0\n",
            "created_at            0\n",
            "salt                  0\n",
            "birthday_on     3294901\n",
            "gender          1317523\n",
            "dtype: int64\n",
            "Invalid email addresses saved to: /content/dump/invalid_emails.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: I have a Data Frame with header names in lowercase. convert the first letter of each header to uppercase and update this code\n",
        "\n",
        "import pandas as pd\n",
        "import os\n",
        "import re\n",
        "\n",
        "def split_csv(file_path, chunk_size, output_directory):\n",
        "    \"\"\"\n",
        "    Splits a CSV file into multiple chunks based on the specified chunk size.\n",
        "\n",
        "    Args:\n",
        "        file_path (str): Path to the input CSV file.\n",
        "        chunk_size (int): Number of rows per chunk.\n",
        "        output_directory (str): Directory to save the output chunks.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        if not os.path.exists(output_directory):\n",
        "            os.makedirs(output_directory)\n",
        "\n",
        "        df = pd.read_csv(file_path, sep=';', low_memory=True, encoding='latin-1')  # Specify encoding if needed\n",
        "\n",
        "        # Remove duplicate rows based on all columns\n",
        "        df = df.drop_duplicates()\n",
        "\n",
        "        # Check for missing values in the DataFrame before splitting\n",
        "        missing_values = df.isnull().sum()\n",
        "        print(\"\\nMissing values per column:\\n\", missing_values)\n",
        "\n",
        "        # Convert column names to title case (first letter uppercase)\n",
        "        new_columns = {col: col.title() for col in df.columns}\n",
        "        df.rename(columns=new_columns, inplace=True)\n",
        "\n",
        "        num_chunks = (len(df) // chunk_size) + 1\n",
        "\n",
        "        for i in range(num_chunks):\n",
        "            start_row = i * chunk_size\n",
        "            end_row = min((i + 1) * chunk_size, len(df))\n",
        "            chunk = df.iloc[start_row:end_row]\n",
        "\n",
        "            output_file = os.path.join(output_directory, f\"chunk_{i+1}.csv\")\n",
        "            chunk.to_csv(output_file, index=False)\n",
        "\n",
        "            print(f\"Chunk {i+1} saved to: {output_file}\")\n",
        "\n",
        "    except FileNotFoundError:\n",
        "        print(f\"Error: File not found at {file_path}\")\n",
        "    except Exception as e:\n",
        "        print(f\"An error occurred: {e}\")\n",
        "\n",
        "\n",
        "# Example usage:\n",
        "file_path = \"/content/3.6M-Japan-lifebear.com-Largest-Notebook-App-UsersDB-csv-2019.csv\"\n",
        "chunk_size = 500000  # 1 million rows per chunk\n",
        "output_directory = \"/content/chunks\"\n",
        "\n",
        "split_csv(file_path, chunk_size, output_directory)\n",
        "\n",
        "\n",
        "\n",
        "# Read the CSV file with specified separator and low_memory option\n",
        "df = pd.read_csv('/content/3.6M-Japan-lifebear.com-Largest-Notebook-App-UsersDB-csv-2019.csv', sep=';', low_memory=True)\n",
        "\n",
        "# Print the first 5 rows of the DataFrame\n",
        "print(df.head())\n",
        "\n",
        "\n",
        "# Check for missing values in the DataFrame\n",
        "missing_values = df.isnull().sum()\n",
        "print(\"\\nMissing values per column:\\n\", missing_values)\n",
        "\n",
        "# Function to check if an email is valid\n",
        "def is_valid_email(email):\n",
        "    if not isinstance(email, str):\n",
        "        return False\n",
        "    email_regex = r\"^[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\\.[a-zA-Z]{2,}$\"\n",
        "    return bool(re.match(email_regex, email))\n",
        "\n",
        "\n",
        "# Create a folder named 'dump' if it doesn't exist\n",
        "dump_folder = '/content/dump'\n",
        "if not os.path.exists(dump_folder):\n",
        "    os.makedirs(dump_folder)\n",
        "\n",
        "# Iterate through the DataFrame and find invalid email addresses in the 'mail_address' column\n",
        "# Rename 'mail_address' to 'Email Address' if it exists\n",
        "if 'mail_address' in df.columns:\n",
        "  df.rename(columns={'mail_address': 'Email Address'}, inplace=True)\n",
        "\n",
        "# Use 'Email Address' for finding invalid emails\n",
        "if 'Email Address' in df.columns:\n",
        "  invalid_emails_df = df[~df['Email Address'].apply(is_valid_email)]\n",
        "\n",
        "  # Save the rows with invalid emails to a CSV file in the 'dump' folder\n",
        "  invalid_emails_df.to_csv(os.path.join(dump_folder, 'invalid_emails.csv'), index=False)\n",
        "\n",
        "  print(f\"Invalid email addresses saved to: {os.path.join(dump_folder, 'invalid_emails.csv')}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u2qAoJKFl3hh",
        "outputId": "3e3a57bc-e17d-4b78-eb7b-60e7b2a351f7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-16-b21ae71d4c3a>:20: DtypeWarning: Columns (6) have mixed types. Specify dtype option on import or set low_memory=False.\n",
            "  df = pd.read_csv(file_path, sep=';', low_memory=True, encoding='latin-1')  # Specify encoding if needed\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Missing values per column:\n",
            " id                    0\n",
            "login_id              2\n",
            "mail_address          0\n",
            "password              0\n",
            "created_at            0\n",
            "salt                  0\n",
            "birthday_on     3294901\n",
            "gender          1317523\n",
            "dtype: int64\n",
            "Chunk 1 saved to: /content/chunks/chunk_1.csv\n",
            "Chunk 2 saved to: /content/chunks/chunk_2.csv\n",
            "Chunk 3 saved to: /content/chunks/chunk_3.csv\n",
            "Chunk 4 saved to: /content/chunks/chunk_4.csv\n",
            "Chunk 5 saved to: /content/chunks/chunk_5.csv\n",
            "Chunk 6 saved to: /content/chunks/chunk_6.csv\n",
            "Chunk 7 saved to: /content/chunks/chunk_7.csv\n",
            "Chunk 8 saved to: /content/chunks/chunk_8.csv\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-16-b21ae71d4c3a>:61: DtypeWarning: Columns (6) have mixed types. Specify dtype option on import or set low_memory=False.\n",
            "  df = pd.read_csv('/content/3.6M-Japan-lifebear.com-Largest-Notebook-App-UsersDB-csv-2019.csv', sep=';', low_memory=True)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   id    login_id              mail_address                          password  \\\n",
            "0   1    sugimoto   sugimoto@lifebear.co.jp  f0bac04aa1b45cf443d722d6f71c0250   \n",
            "1   2         kou  nakanishi@lifebear.co.jp  48207c322ee5bb156ffec9f08c960aaa   \n",
            "2   3      yusuke     yuozawa1208@gmail.com  048261a8024ce51d379eb53cc51aaf33   \n",
            "3   4  entyan1106        endo1106@gmail.com  cd77a9dac26260a104facda5665eb3ab   \n",
            "4   5      kuriki          kuriki@wavy4.com  a026597c294cc48cd20ae361f10cbab1   \n",
            "\n",
            "            created_at          salt birthday_on  gender  \n",
            "0  2012-01-13 22:54:05  yGwBKynnsctI  1984-11-09     0.0  \n",
            "1  2012-01-14 12:48:31  aha6EuRYCDvU  1986-11-13     0.0  \n",
            "2  2012-01-17 15:33:22  PVS59dPWk9BH  1984-12-08     0.0  \n",
            "3  2012-01-17 15:37:02  vLZI6TVCJowN  1987-11-06     0.0  \n",
            "4  2012-01-17 18:52:32  swFznWWk79fg  1986-10-21     0.0  \n",
            "\n",
            "Missing values per column:\n",
            " id                    0\n",
            "login_id              2\n",
            "mail_address          0\n",
            "password              0\n",
            "created_at            0\n",
            "salt                  0\n",
            "birthday_on     3294901\n",
            "gender          1317523\n",
            "dtype: int64\n",
            "Invalid email addresses saved to: /content/dump/invalid_emails.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt:  update this code to do a  Final cleaned data preview using these functions\n",
        "# print(\"\\nCleaned Data:\")\n",
        "# print(df_cleaned.head())\n",
        "#  print the 1st 5 and the last 5.\n",
        "\n",
        "import pandas as pd\n",
        "import os\n",
        "import re\n",
        "\n",
        "# ... (Your existing code) ...\n",
        "\n",
        "\n",
        "def split_csv(file_path, chunk_size, output_directory):\n",
        "    \"\"\"\n",
        "    Splits a CSV file into multiple chunks based on the specified chunk size.\n",
        "\n",
        "    Args:\n",
        "        file_path (str): Path to the input CSV file.\n",
        "        chunk_size (int): Number of rows per chunk.\n",
        "        output_directory (str): Directory to save the output chunks.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        if not os.path.exists(output_directory):\n",
        "            os.makedirs(output_directory)\n",
        "\n",
        "        df = pd.read_csv(file_path, sep=';', low_memory=True, encoding='latin-1')  # Specify encoding if needed\n",
        "\n",
        "        # Remove duplicate rows based on all columns\n",
        "        df = df.drop_duplicates()\n",
        "\n",
        "        # Check for missing values in the DataFrame before splitting\n",
        "        missing_values = df.isnull().sum()\n",
        "        print(\"\\nMissing values per column:\\n\", missing_values)\n",
        "\n",
        "        # Convert column names to title case (first letter uppercase)\n",
        "        new_columns = {col: col.title() for col in df.columns}\n",
        "        df.rename(columns=new_columns, inplace=True)\n",
        "\n",
        "        num_chunks = (len(df) // chunk_size) + 1\n",
        "\n",
        "        for i in range(num_chunks):\n",
        "            start_row = i * chunk_size\n",
        "            end_row = min((i + 1) * chunk_size, len(df))\n",
        "            chunk = df.iloc[start_row:end_row]\n",
        "\n",
        "            output_file = os.path.join(output_directory, f\"chunk_{i+1}.csv\")\n",
        "            chunk.to_csv(output_file, index=False)\n",
        "\n",
        "            print(f\"Chunk {i+1} saved to: {output_file}\")\n",
        "\n",
        "    except FileNotFoundError:\n",
        "        print(f\"Error: File not found at {file_path}\")\n",
        "    except Exception as e:\n",
        "        print(f\"An error occurred: {e}\")\n",
        "\n",
        "\n",
        "# Example usage:\n",
        "file_path = \"/content/3.6M-Japan-lifebear.com-Largest-Notebook-App-UsersDB-csv-2019.csv\"\n",
        "chunk_size = 500000  # 1 million rows per chunk\n",
        "output_directory = \"/content/chunks\"\n",
        "\n",
        "split_csv(file_path, chunk_size, output_directory)\n",
        "\n",
        "\n",
        "\n",
        "# Read the CSV file with specified separator and low_memory option\n",
        "df = pd.read_csv('/content/3.6M-Japan-lifebear.com-Largest-Notebook-App-UsersDB-csv-2019.csv', sep=';', low_memory=True)\n",
        "\n",
        "# ... (Your existing code) ...\n",
        "\n",
        "# Final cleaned data preview\n",
        "if 'Email Address' in df.columns:\n",
        "    df_cleaned = df.copy()  # Make a copy to avoid modifying the original DataFrame\n",
        "\n",
        "    # Assuming you've done other cleaning steps, you can add them here\n",
        "    # For example:\n",
        "    # df_cleaned = df_cleaned.dropna(subset=['Email Address'])  # Remove rows with missing email addresses\n",
        "\n",
        "    print(\"\\nCleaned Data (First 5 rows):\")\n",
        "    print(df_cleaned.head(5))\n",
        "\n",
        "    print(\"\\nCleaned Data (Last 5 rows):\")\n",
        "    print(df_cleaned.tail(5))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wTrznQnXmbE-",
        "outputId": "5fb9c84d-ae80-4082-858d-98fd7700ebef"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-18-2a45c187ea5a>:26: DtypeWarning: Columns (6) have mixed types. Specify dtype option on import or set low_memory=False.\n",
            "  df = pd.read_csv(file_path, sep=';', low_memory=True, encoding='latin-1')  # Specify encoding if needed\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Missing values per column:\n",
            " id                    0\n",
            "login_id              2\n",
            "mail_address          0\n",
            "password              0\n",
            "created_at            0\n",
            "salt                  0\n",
            "birthday_on     3294901\n",
            "gender          1317523\n",
            "dtype: int64\n",
            "Chunk 1 saved to: /content/chunks/chunk_1.csv\n",
            "Chunk 2 saved to: /content/chunks/chunk_2.csv\n",
            "Chunk 3 saved to: /content/chunks/chunk_3.csv\n",
            "Chunk 4 saved to: /content/chunks/chunk_4.csv\n",
            "Chunk 5 saved to: /content/chunks/chunk_5.csv\n",
            "Chunk 6 saved to: /content/chunks/chunk_6.csv\n",
            "Chunk 7 saved to: /content/chunks/chunk_7.csv\n",
            "Chunk 8 saved to: /content/chunks/chunk_8.csv\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-18-2a45c187ea5a>:67: DtypeWarning: Columns (6) have mixed types. Specify dtype option on import or set low_memory=False.\n",
            "  df = pd.read_csv('/content/3.6M-Japan-lifebear.com-Largest-Notebook-App-UsersDB-csv-2019.csv', sep=';', low_memory=True)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: update this code to merge all the chunks back together as one data frame and save as a csv called \"merge life bear clean data\"\n",
        "\n",
        "import pandas as pd\n",
        "import os\n",
        "import re\n",
        "\n",
        "def split_csv(file_path, chunk_size, output_directory):\n",
        "    \"\"\"\n",
        "    Splits a CSV file into multiple chunks based on the specified chunk size.\n",
        "\n",
        "    Args:\n",
        "        file_path (str): Path to the input CSV file.\n",
        "        chunk_size (int): Number of rows per chunk.\n",
        "        output_directory (str): Directory to save the output chunks.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        if not os.path.exists(output_directory):\n",
        "            os.makedirs(output_directory)\n",
        "\n",
        "        df = pd.read_csv(file_path, sep=';', low_memory=True, encoding='latin-1')  # Specify encoding if needed\n",
        "\n",
        "        # Remove duplicate rows based on all columns\n",
        "        df = df.drop_duplicates()\n",
        "\n",
        "        # Check for missing values in the DataFrame before splitting\n",
        "        missing_values = df.isnull().sum()\n",
        "        print(\"\\nMissing values per column:\\n\", missing_values)\n",
        "\n",
        "        # Convert column names to title case (first letter uppercase)\n",
        "        new_columns = {col: col.title() for col in df.columns}\n",
        "        df.rename(columns=new_columns, inplace=True)\n",
        "\n",
        "        num_chunks = (len(df) // chunk_size) + 1\n",
        "\n",
        "        for i in range(num_chunks):\n",
        "            start_row = i * chunk_size\n",
        "            end_row = min((i + 1) * chunk_size, len(df))\n",
        "            chunk = df.iloc[start_row:end_row]\n",
        "\n",
        "            output_file = os.path.join(output_directory, f\"chunk_{i+1}.csv\")\n",
        "            chunk.to_csv(output_file, index=False)\n",
        "\n",
        "            print(f\"Chunk {i+1} saved to: {output_file}\")\n",
        "\n",
        "    except FileNotFoundError:\n",
        "        print(f\"Error: File not found at {file_path}\")\n",
        "    except Exception as e:\n",
        "        print(f\"An error occurred: {e}\")\n",
        "\n",
        "\n",
        "# Example usage:\n",
        "file_path = \"/content/3.6M-Japan-lifebear.com-Largest-Notebook-App-UsersDB-csv-2019.csv\"\n",
        "chunk_size = 500000  # 1 million rows per chunk\n",
        "output_directory = \"/content/chunks\"\n",
        "\n",
        "split_csv(file_path, chunk_size, output_directory)\n",
        "\n",
        "\n",
        "\n",
        "# Read the CSV file with specified separator and low_memory option\n",
        "df = pd.read_csv('/content/3.6M-Japan-lifebear.com-Largest-Notebook-App-UsersDB-csv-2019.csv', sep=';', low_memory=True)\n",
        "\n",
        "# Print the first 5 rows of the DataFrame\n",
        "print(df.head())\n",
        "\n",
        "\n",
        "# Check for missing values in the DataFrame\n",
        "missing_values = df.isnull().sum()\n",
        "print(\"\\nMissing values per column:\\n\", missing_values)\n",
        "\n",
        "# Function to check if an email is valid\n",
        "def is_valid_email(email):\n",
        "    if not isinstance(email, str):\n",
        "        return False\n",
        "    email_regex = r\"^[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\\.[a-zA-Z]{2,}$\"\n",
        "    return bool(re.match(email_regex, email))\n",
        "\n",
        "\n",
        "# Create a folder named 'dump' if it doesn't exist\n",
        "dump_folder = '/content/dump'\n",
        "if not os.path.exists(dump_folder):\n",
        "    os.makedirs(dump_folder)\n",
        "\n",
        "# Iterate through the DataFrame and find invalid email addresses in the 'mail_address' column\n",
        "# Rename 'mail_address' to 'Email Address' if it exists\n",
        "if 'mail_address' in df.columns:\n",
        "  df.rename(columns={'mail_address': 'Email Address'}, inplace=True)\n",
        "\n",
        "# Use 'Email Address' for finding invalid emails\n",
        "if 'Email Address' in df.columns:\n",
        "  invalid_emails_df = df[~df['Email Address'].apply(is_valid_email)]\n",
        "\n",
        "  # Save the rows with invalid emails to a CSV file in the 'dump' folder\n",
        "  invalid_emails_df.to_csv(os.path.join(dump_folder, 'invalid_emails.csv'), index=False)\n",
        "\n",
        "  print(f\"Invalid email addresses saved to: {os.path.join(dump_folder, 'invalid_emails.csv')}\")\n",
        "\n",
        "def merge_chunks(output_directory, output_file_name):\n",
        "    \"\"\"\n",
        "    Merges multiple CSV chunks into a single DataFrame and saves it as a CSV file.\n",
        "\n",
        "    Args:\n",
        "        output_directory (str): Directory containing the CSV chunks.\n",
        "        output_file_name (str): Name of the output CSV file.\n",
        "    \"\"\"\n",
        "\n",
        "    all_chunks = []\n",
        "    for filename in os.listdir(output_directory):\n",
        "        if filename.startswith(\"chunk_\") and filename.endswith(\".csv\"):\n",
        "            file_path = os.path.join(output_directory, filename)\n",
        "            chunk_df = pd.read_csv(file_path, low_memory=False)\n",
        "            all_chunks.append(chunk_df)\n",
        "\n",
        "    merged_df = pd.concat(all_chunks, ignore_index=True)\n",
        "    merged_df.to_csv(output_file_name, index=False)\n",
        "\n",
        "    print(f\"Merged chunks saved to: {output_file_name}\")\n",
        "\n",
        "# Example usage to merge chunks:\n",
        "output_directory = \"/content/chunks\"  # Same as your output directory for chunks\n",
        "output_file_name = \"merge life bear clean data.csv\"\n",
        "merge_chunks(output_directory, output_file_name)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Kvmr9QlFMuqD",
        "outputId": "3201d101-829a-4648-a06f-f07af6466539"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-19-e54282eed862>:20: DtypeWarning: Columns (6) have mixed types. Specify dtype option on import or set low_memory=False.\n",
            "  df = pd.read_csv(file_path, sep=';', low_memory=True, encoding='latin-1')  # Specify encoding if needed\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Missing values per column:\n",
            " id                    0\n",
            "login_id              2\n",
            "mail_address          0\n",
            "password              0\n",
            "created_at            0\n",
            "salt                  0\n",
            "birthday_on     3294901\n",
            "gender          1317523\n",
            "dtype: int64\n",
            "Chunk 1 saved to: /content/chunks/chunk_1.csv\n",
            "Chunk 2 saved to: /content/chunks/chunk_2.csv\n",
            "Chunk 3 saved to: /content/chunks/chunk_3.csv\n",
            "Chunk 4 saved to: /content/chunks/chunk_4.csv\n",
            "Chunk 5 saved to: /content/chunks/chunk_5.csv\n",
            "Chunk 6 saved to: /content/chunks/chunk_6.csv\n",
            "Chunk 7 saved to: /content/chunks/chunk_7.csv\n",
            "Chunk 8 saved to: /content/chunks/chunk_8.csv\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-19-e54282eed862>:61: DtypeWarning: Columns (6) have mixed types. Specify dtype option on import or set low_memory=False.\n",
            "  df = pd.read_csv('/content/3.6M-Japan-lifebear.com-Largest-Notebook-App-UsersDB-csv-2019.csv', sep=';', low_memory=True)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   id    login_id              mail_address                          password  \\\n",
            "0   1    sugimoto   sugimoto@lifebear.co.jp  f0bac04aa1b45cf443d722d6f71c0250   \n",
            "1   2         kou  nakanishi@lifebear.co.jp  48207c322ee5bb156ffec9f08c960aaa   \n",
            "2   3      yusuke     yuozawa1208@gmail.com  048261a8024ce51d379eb53cc51aaf33   \n",
            "3   4  entyan1106        endo1106@gmail.com  cd77a9dac26260a104facda5665eb3ab   \n",
            "4   5      kuriki          kuriki@wavy4.com  a026597c294cc48cd20ae361f10cbab1   \n",
            "\n",
            "            created_at          salt birthday_on  gender  \n",
            "0  2012-01-13 22:54:05  yGwBKynnsctI  1984-11-09     0.0  \n",
            "1  2012-01-14 12:48:31  aha6EuRYCDvU  1986-11-13     0.0  \n",
            "2  2012-01-17 15:33:22  PVS59dPWk9BH  1984-12-08     0.0  \n",
            "3  2012-01-17 15:37:02  vLZI6TVCJowN  1987-11-06     0.0  \n",
            "4  2012-01-17 18:52:32  swFznWWk79fg  1986-10-21     0.0  \n",
            "\n",
            "Missing values per column:\n",
            " id                    0\n",
            "login_id              2\n",
            "mail_address          0\n",
            "password              0\n",
            "created_at            0\n",
            "salt                  0\n",
            "birthday_on     3294901\n",
            "gender          1317523\n",
            "dtype: int64\n",
            "Invalid email addresses saved to: /content/dump/invalid_emails.csv\n",
            "Merged chunks saved to: merge life bear clean data.csv\n"
          ]
        }
      ]
    }
  ]
}